{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions to perform 30x4-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from memoization import cached\n",
    "import scikit_posthocs as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%run ../../helper_modules/run_or_load.py\n",
    "%run ../../helper_modules/plotting_metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memoization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory memoization\n",
    "from joblib import Memory\n",
    "location = './cachedir'\n",
    "memory = Memory(location, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cached\n",
    "def order_df(df, metric_name, ascending=True):\n",
    "    order = df.loc[metric_name].mean().sort_values(ascending=ascending)\n",
    "    df = df.reindex(order.index, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_swarm_metrics(df_results, metric_name, \n",
    "                       order_cols=True, title_extra='', \n",
    "                       ref_values = None,\n",
    "                       **kwargs):\n",
    "    sns.set(context='talk', style='whitegrid', font_scale=0.8)\n",
    "    \n",
    "    if order_cols:\n",
    "        df_results = order_df(df_results, metric_name, **kwargs) \n",
    "    \n",
    "    df_melted = pd.melt(df_results.loc[metric_name], var_name='method', value_name=metric_name)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax = sns.swarmplot(data=df_melted, x='method', y=metric_name,\n",
    "                      size=6)\n",
    "    sns.pointplot(data=df_melted, x='method', y=metric_name, dodge=True,\n",
    "                  ci=95, ax=ax, join=False, color='black', capsize=0.4)\n",
    "    ax.set(\n",
    "           xlabel='VS Method')\n",
    "    ax.set_title(f'ML vs CS results: {metric_name.replace(\"_\", \" \").upper()}_{title_extra}', \n",
    "                 fontweight='bold', fontsize=18)\n",
    "    if metric_name == 'roc_auc':\n",
    "        ax.set_ylim(0.3, 1.001)\n",
    "        ax.axhline(0.5, ls='--')\n",
    "    else:\n",
    "        ax.set_ylim(0.0, 1.001)\n",
    "    # plot horizontal lines with reference values\n",
    "    if ref_values != None and isinstance(ref_values, dict):\n",
    "        for name, value in ref_values:\n",
    "            ax.axhline(value, ls='--', c='powderblue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, RepeatedStratifiedKFold\n",
    "\n",
    "def _validate(clf, clf_name, \n",
    "                X_test, y_test,\n",
    "                metric_name, metric_params):\n",
    "    if clf_name.startswith('ml_'):\n",
    "        # Make the predictions\n",
    "        y_pred = clf.predict_proba(X_test)[:,1]\n",
    "    elif clf_name.startswith('cs_'):\n",
    "        # apply consensuss we\n",
    "        y_pred = clf(pd.DataFrame(X_test))\n",
    "    else:\n",
    "        print(clf_name, 'not found. Ommited.')\n",
    "        return\n",
    "    \n",
    "    # Make the evaluation\n",
    "    metric = PlotMetric(y_test, pd.DataFrame({'y_pred': y_pred}),\n",
    "                decreasing=False)\\\n",
    "                .format_metric_results(\n",
    "                    rounded=5,\n",
    "                    **metric_params)\n",
    "\n",
    "    return metric.values[0][0], y_pred\n",
    "\n",
    "\n",
    "def _train_cfl(clf, X_train, y_train):\n",
    "    # Fit the estimator\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "    \n",
    "\n",
    "def _do_replicates(splits, \n",
    "                   estimators, X, y,\n",
    "                   metrics,\n",
    "                   random_vars=False,\n",
    "                   k_random_vars=0\n",
    "                  ):\n",
    "    results={}\n",
    "    y_preds_dict={}\n",
    "    # Machine Learning Classifiers\n",
    "    for clf_name, clf in estimators.items():\n",
    "        folds = []\n",
    "        y_preds = []\n",
    "        for i, (train, test) in enumerate(splits):\n",
    "            if clf_name.startswith('ml_'):\n",
    "                \n",
    "                # Subset k random variables?\n",
    "                if random_vars and k_random_vars > 0:\n",
    "                    n_ = X.shape[1]\n",
    "                    vars_index = np.random.choice(\n",
    "                            a = n_,\n",
    "                            size = k_random_vars,\n",
    "                            replace = False\n",
    "                        )\n",
    "                    #Subset X by columns\n",
    "                    X = X[:, vars_index]\n",
    "                \n",
    "                \n",
    "                # Fit the ml classifier once per fold\n",
    "                cfl = _train_cfl(clf, X[train], y[train])\n",
    "            \n",
    "            \n",
    "            for metric_name, metric_params in metrics.items():\n",
    "                metric, y_pred = _validate(\n",
    "                    clf, clf_name, \n",
    "                    X[test], y[test],\n",
    "                    metric_name, metric_params\n",
    "                )\n",
    "                # Append the results\n",
    "                folds.append(metric)\n",
    "            # Append the clf prediction (should be the same for ec metric)\n",
    "            # One prediction per fold\n",
    "            y_preds.append(y_pred)\n",
    "            \n",
    "        # Add to the dictionary the k predictions of the clf\n",
    "        y_preds_dict[clf_name] = y_preds\n",
    "            \n",
    "        # Add to the results dictonary \n",
    "        results[clf_name] = folds\n",
    "\n",
    "    return results, y_preds_dict\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def _format_results_to_df(metrics, results, n):\n",
    "        # Format into a dataframe\n",
    "    # Create the metric names and repeat them \n",
    "    n_metrics = len(metrics)\n",
    "    index_names = [*metrics.keys()]*n\n",
    "    \n",
    "    # convert to a dataframe\n",
    "    df_res = pd.DataFrame(\n",
    "        results, \n",
    "        index= pd.MultiIndex.from_tuples(\n",
    "            zip(index_names,\n",
    "                np.repeat(range(n), n_metrics))\n",
    "        ))\n",
    "    df_res = df_res.sort_index()\n",
    "    \n",
    "    return df_res\n",
    "\n",
    "@memory.cache\n",
    "def k_cross_validation(\n",
    "          estimators, X, y,\n",
    "          metrics,\n",
    "          n_splits=5, \n",
    "          random_state=None, \n",
    "          shuffle=True, y_preds_return=False):\n",
    "    # Compute the Stratified K folds\n",
    "    cv = StratifiedKFold(n_splits=n_splits, \n",
    "                         random_state=random_state,\n",
    "                         shuffle=shuffle)\n",
    "    splits = [*cv.split(X, y)]\n",
    "    \n",
    "    results, y_preds_dict = _do_replicates(splits, estimators, X, y, \n",
    "                             metrics)\n",
    "    \n",
    "    df_res = _format_results_to_df(metrics, results, n=n_splits)\n",
    "    \n",
    "    if y_preds_return:\n",
    "        return df_res, y_preds_dict, splits #Test indexes\n",
    "    else:\n",
    "        return df_res \n",
    "\n",
    "@memory.cache\n",
    "def n_hold_out_validation(\n",
    "          estimators, X, y,\n",
    "          metrics,\n",
    "          n_reps=5, test_size=0.25,\n",
    "          random_state=None, y_preds_return=False, **kwargs):\n",
    "    # Compute the Stratified K folds\n",
    "    cv = StratifiedShuffleSplit(\n",
    "                        n_splits=n_reps, \n",
    "                        test_size=test_size,\n",
    "                        random_state=random_state)\n",
    "    splits = [*cv.split(X, y)]\n",
    "    \n",
    "    results, y_preds_dict = _do_replicates(splits, estimators, X, y,\n",
    "          metrics, **kwargs)\n",
    "    \n",
    "    df_res = _format_results_to_df(metrics, results, n=n_reps)\n",
    "    \n",
    "    if y_preds_return:\n",
    "        return df_res, y_preds_dict, splits #Test idx\n",
    "    else:\n",
    "        return df_res  \n",
    "\n",
    "@memory.cache\n",
    "def nk_rep_cross_validation(\n",
    "          estimators, X, y,\n",
    "          metrics,\n",
    "          n_splits=2, \n",
    "          n_repeats=5,\n",
    "          random_state=None, \n",
    "          shuffle=True,  y_preds_return=False):\n",
    "    # Compute the Stratified K folds\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "                         n_splits=n_splits,\n",
    "                         n_repeats=n_repeats,\n",
    "                         random_state=random_state)\n",
    "    splits = [*cv.split(X, y)]\n",
    "    \n",
    "    results, y_preds_dict = _do_replicates(splits, \n",
    "                                estimators, X, y, \n",
    "                                metrics)\n",
    "    \n",
    "    df_res = _format_results_to_df(metrics, results, \n",
    "                                   n=n_splits*n_repeats)\n",
    "    \n",
    "    if y_preds_return:\n",
    "        return df_res, y_preds_dict, splits  # Test idx\n",
    "    else:\n",
    "        return df_res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_or_load_joblib\n",
    "def n_hold_out_validation_RANDOM_CONFS_SAVE(filename, n_reps, k, max_confs, \n",
    "                                            X, y, metrics, **kwargs):\n",
    "    results_list = []\n",
    "    for i in range(n_reps):\n",
    "            # Get the conformations randomly\n",
    "            conformations = np.random.choice(max_confs, size=k, replace=None)\n",
    "            X_sub = X[:, conformations]\n",
    "            # Do the analysis\n",
    "            result_i = n_hold_out_validation(\n",
    "                         estimators=estimators, \n",
    "                         X=X_sub, y=y, metrics=metrics, \n",
    "                         n_reps=1, random_state=None\n",
    "                    )\n",
    "            # Edit the rep number value: increment = +1\n",
    "            result_i.index = result_i.index.set_levels([i], level=1)\n",
    "            # Append to the results_list\n",
    "            results_list.append(result_i)\n",
    "        \n",
    "     # Concat the n results\n",
    "    result_end = pd.concat(results_list, axis=0).sort_index()\n",
    "    return result_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_or_load_joblib\n",
    "def nk_rep_cross_validation_RANDOM_CONFS_SAVE(\n",
    "                              filename, X, y, \n",
    "                              n_reps, n_splits,\n",
    "                              k, max_confs, \n",
    "                              metrics,\n",
    "                              random_state=None,\n",
    "                              **kwargs):\n",
    "    results_list = []\n",
    "    for i in range(n_reps):\n",
    "            # Get the conformations randomly\n",
    "            conformations = np.random.choice(max_confs, \n",
    "                                             size=k, replace=None)\n",
    "            X_sub = X[:, conformations]\n",
    "            # Do the analysis\n",
    "            result_i = nk_rep_cross_validation(\n",
    "                         X=X_sub, y=y, \n",
    "                         estimators=estimators, \n",
    "                         metrics=metrics, \n",
    "                         n_repeats=1, n_splits=n_splits,\n",
    "                         random_state=random_state,\n",
    "                         y_preds_return=False\n",
    "                    )\n",
    "            # Edit the rep number value: increment = +1\n",
    "            result_i.index = result_i.index.set_levels(\n",
    "            list(np.arange(n_splits) + (i*n_splits)), level=1)\n",
    "            # Append to the results_list\n",
    "            results_list.append(result_i)\n",
    "        \n",
    "     # Concat the n results\n",
    "    result_end = pd.concat(results_list, axis=0).sort_index()\n",
    "    return result_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_or_load_joblib\n",
    "def nk_rep_cross_validation_scrambling(filename, y, \n",
    "                random_chi, n_repeats, n_splits, **kwargs):\n",
    "    results_list = []\n",
    "    for i in range(n_repeats):\n",
    "        y_rand = randomize_y_labels(y_target=y, \n",
    "                                    random_chi=random_chi)\n",
    "        result_i = nk_rep_cross_validation(\n",
    "                            y=y_rand, \n",
    "                            n_repeats=1,\n",
    "                             n_splits=n_splits,\n",
    "                             random_state=None,\n",
    "                            y_preds_return=False,\n",
    "                            **kwargs\n",
    "                    )\n",
    "        result_i.index = result_i.index.set_levels(\n",
    "            list(np.arange(n_splits) + (i*n_splits)), level=1)\n",
    "        # Append to the results_list\n",
    "        results_list.append(result_i)\n",
    "        \n",
    "    # Concat the n results\n",
    "    result_end = pd.concat(results_list, axis=0).sort_index()\n",
    "    return result_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the best and the mean single score\n",
    "@cached\n",
    "def get_best_single_performance(y_true, X, metric_params, decreasing=True):\n",
    "    preds = pd.DataFrame({i:j for i,j in zip(range(X.shape[1]), X.T)})\n",
    "    performances = PlotMetric(y_true,\n",
    "                              preds,\n",
    "                decreasing=decreasing)\\\n",
    "                .format_metric_results(\n",
    "                    rounded=5,\n",
    "                    **metric_params)\n",
    "    mean = performances.mean()\n",
    "    maximum = performances.max()\n",
    "    return mean.values[0], maximum.values[0]\n",
    "\n",
    "@cached\n",
    "def n_hold_out_single_performance(\n",
    "          X, y,\n",
    "          metric,\n",
    "          n_reps=5, test_size=0.25,\n",
    "          random_state=None, decreasing=True):\n",
    "    # Compute the Stratified K folds\n",
    "    cv = StratifiedShuffleSplit(\n",
    "                        n_splits=n_reps, \n",
    "                        test_size=test_size,\n",
    "                        random_state=random_state)\n",
    "    splits = [*cv.split(X, y)]\n",
    "    \n",
    "    rep_mean = []\n",
    "    rep_max = []\n",
    "    # We are only interested in the test set\n",
    "    for _, test in splits:\n",
    "        mean, maximum = get_best_single_performance(y[test], X[test], \n",
    "                                                    metric, decreasing=decreasing)\n",
    "        rep_mean.append(mean)\n",
    "        rep_max.append(maximum)\n",
    "    # Compute the final results: One mean and one maximum\n",
    "    final_mean = np.mean(rep_mean)\n",
    "    final_max = np.max(rep_max)\n",
    "    \n",
    "    return final_mean, final_max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "@cached()\n",
    "def plot_roc_cv(classifier, X, y, random_state=None,\n",
    "               n_folds=5, ax=None, name=''):\n",
    "    sns.set(style='whitegrid', font_scale=1.2)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots()\n",
    "    for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        classifier.fit(X[train], y[train])\n",
    "        viz = plot_roc_curve(classifier, X[test], y[test],\n",
    "                             name='ROC fold {}'.format(i),\n",
    "                             alpha=0.3, lw=1, ax=ax)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "            label='Random', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "    ax.set_title(label=f\"{n_folds}-fold CV ROC curve: {name}\", fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro, bartlett, levene\n",
    "\n",
    "def norm_test(x, alpha=0.05):\n",
    "    s, p = shapiro(x)\n",
    "    result = 'rejected' if p < alpha else 'accepted'\n",
    "    print(f'The H0 is {result} => (W={round(s, 3)}, p={round(p, 3)})')\n",
    "    \n",
    "def homovar_test(x, y, alpha=0.05):\n",
    "    s, p = bartlett(x, y)\n",
    "    result = 'rejected' if p < alpha else 'accepted'\n",
    "    print(f'The H0 is {result} => (W={round(s, 3)}, p={round(p, 3)})')\n",
    "\n",
    "def multi_norm_test(df, metric='roc_auc', alpha=0.05):\n",
    "    res = df.loc[metric].apply(shapiro, axis=0)\n",
    "    return pd.DataFrame([0 if i < alpha else 1 \n",
    "                         for i in res.T.iloc[:,1]], \n",
    "             index=df.columns, columns=['Normality']).T\n",
    "\n",
    "def multi_norm_test_values(df, metric='roc_auc', alpha=0.05):\n",
    "    res = df.loc[metric].apply(shapiro, axis=0)\n",
    "    return res.round(3)\n",
    "\n",
    "def multi_homovar_test(df, metric='roc_auc', alpha=0.05, as_df=True):\n",
    "    res = bartlett(*df.loc[metric].values.T)\n",
    "    if as_df:\n",
    "        res = pd.DataFrame(res, columns=['Bartlett'], \n",
    "                           index=['statistic', 'p']).T\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_descriptive_stats(cvnxk, metric_name):\n",
    "    # Long format\n",
    "    df_R = order_df(cv30x4, \n",
    "                metric_name).loc[metric_name]\n",
    "    # Descriptive statistics\n",
    "    medians = df_R.median(axis=0)\\\n",
    "                  .to_frame(name='medians').T\n",
    "    print('Meadians:')\n",
    "    display(medians)\n",
    "\n",
    "    IQRs = df_R.apply(stats.iqr, \n",
    "                      interpolation = 'midpoint')\\\n",
    "                      .to_frame(name='IQRs').T\n",
    "    print('IQRs:')\n",
    "    display(IQRs)\n",
    "\n",
    "    # Normality\n",
    "    print('Normality test results (Sahpiro-Wilk):')\n",
    "    display(multi_norm_test(cvnxk, \n",
    "                            metric = metric_name))\n",
    "\n",
    "    normality_res = multi_norm_test_values(\n",
    "                            cvnxk, \n",
    "                            metric = metric_name)\n",
    "\n",
    "    # Friedman Test\n",
    "    print('Friedman Test:')\n",
    "    display(friedmanTest(cvnxk.loc[metric_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_norm(df_list =[], names=['AUC-ROC', 'NEF']):\n",
    "    '''Function to obtain a latex table with the normality test results'''\n",
    "    df_res_list = []\n",
    "    for df in df_list:\n",
    "        df.index = ['W-statistic', 'p-value']\n",
    "        df_res_list.append(df.T)\n",
    "    \n",
    "    df_res = pd.concat(df_res_list, axis=1, keys=names)\n",
    "    \n",
    "    return df_res.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Orange\n",
    "\n",
    "def plot_cd(df, width=12):\n",
    "    names = df.columns\n",
    "    n = df.shape[0]\n",
    "    avranks =  get_R(df)\n",
    "    cd = Orange.evaluation.compute_CD(avranks, n, alpha='0.05') \n",
    "    print('Critical Difference:', cd)\n",
    "    ax = Orange.evaluation.graph_ranks(avranks, names, \n",
    "                                       cd=cd, width=width, textspace=1.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_p_heatmap(df):\n",
    "    cmap = ['1', '#fb6a4a',  '#08306b',  '#4292c6', '#c6dbef']\n",
    "    p_values_nemenyi = pairwise_nemenyi(df)[0]\n",
    "    mask = np.zeros_like(p_values_nemenyi, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    ax, _ = sp.sign_plot(p_values_nemenyi - 0.000000001, clip_on= False, cmap=cmap,\n",
    "                     linewidths= 1, linecolor= 'white', mask=mask,\n",
    "                     annot=p_values_nemenyi, fmt='.2f')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannot import add_stat_annotation\n",
    "\n",
    "def plot_box_signif(df, metric_name, **kwargs):\n",
    "    \n",
    "    df = order_df(df, metric_name, **kwargs).loc[metric_name]\n",
    "    p_values_nemenyi = pairwise_nemenyi(df)[0]\n",
    "            \n",
    "    df_melted = pd.melt(df, \n",
    "                        var_name='method', value_name='score')\n",
    "    \n",
    "    box_pairs = [*it.combinations(df.columns, 2)]\n",
    "    sig_p_values = []\n",
    "    sig_box_pairs = []\n",
    "    for pair in box_pairs:\n",
    "    #     print(pair)\n",
    "        p = p_values_nemenyi.loc[pair]\n",
    "        if p <= 0.001:\n",
    "            sig_p_values.append(p)\n",
    "            sig_box_pairs.append(pair)\n",
    "    \n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,8))\n",
    "    ax = sns.boxplot(data=df_melted, x='method', y='score')\n",
    "    ax = sns.swarmplot(data=df_melted, x='method', y='score', size=8)\n",
    "    for i,box in enumerate(ax.artists):\n",
    "        box.set_edgecolor('black')\n",
    "        box.set_facecolor('white')\n",
    "\n",
    "        # iterate over whiskers and median lines\n",
    "        for j in range(6*i,6*(i+1)):\n",
    "             ax.lines[j].set_color('black')\n",
    "                \n",
    "    if metric_name == 'roc_auc':\n",
    "        ax.set_ylim(0.3, 1.001)\n",
    "        ax.axhline(0.5, ls='--')\n",
    "    else:\n",
    "        ax.set_ylim(0.0, 1.001)\n",
    "\n",
    "\n",
    "    test_results = add_stat_annotation(ax, data=df_melted, \n",
    "                                       x='method', y='score',\n",
    "                                       box_pairs=sig_box_pairs,\n",
    "                                       perform_stat_test=False,\n",
    "                                       pvalues=sig_p_values,\n",
    "                                       #test_short_name=test_short_name,\n",
    "                                       text_format='star', verbose=0, loc='outside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_y_labels(y_target, random_chi=0.1):\n",
    "    '''Función para distribuir de forma aleatoria una fracción \n",
    "    chi del vector de etiquetas, de forma estratificada'''\n",
    "    \n",
    "    # Make a copy of the original vector\n",
    "    y_copy = y.copy()\n",
    "    \n",
    "    # Get the number of actives inside the y_target vector\n",
    "    n_actives = y_target.sum()\n",
    "    random_size = np.floor(random_chi * n_actives)\n",
    "    # Initialize the counters\n",
    "    act_count = random_size\n",
    "    inact_count = random_size\n",
    "    \n",
    "    # Create the randomized list of idexes\n",
    "    idx_shuffled = np.random.choice(range(len(y)), len(y), replace=False)\n",
    "    # iterate over idx_shuffled until act and inact counters == 0\n",
    "    for l in idx_shuffled:\n",
    "        if act_count > 0:\n",
    "            if y_copy[l] == 1: # Is active, then change it to inactive\n",
    "                y_copy[l] = 0\n",
    "                act_count = act_count - 1\n",
    "                continue\n",
    "            if inact_count > 0: # If is inactive, change it to active\n",
    "                y_copy[l] = 1\n",
    "                inact_count = inact_count - 1\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "    return(y_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "get_ci = lambda x: st.t.interval(0.95, len(x) - 1,\n",
    "                loc=np.mean(x), scale=st.sem(x))\n",
    "\n",
    "compute_stats = lambda r: pd.Series(\n",
    "        {'mean': r.mean(), \n",
    "         'std': r.std()\n",
    "        }\n",
    "    )\n",
    "\n",
    "def get_group_stats(dict_results, metric_name,\n",
    "                   indexes_values = [100, 75, 50, 25, 0]\n",
    "                   ):\n",
    "    stats_df = []\n",
    "\n",
    "    for chi in dict_results.keys():\n",
    "        df =  dict_results[chi].loc[metric_name]\n",
    "        stats = df.apply(compute_stats)\n",
    "        stats_df.append(stats)\n",
    "        \n",
    "    means = pd.concat(stats_df, keys=dict_results.keys()).loc[(slice(None), 'mean'),:].droplevel(1)\n",
    "    stds = pd.concat(stats_df, keys=dict_results.keys()).loc[(slice(None), 'std'),:].droplevel(1)\n",
    "    \n",
    "    # Set index\n",
    "    means.index = indexes_values\n",
    "    stds.index = indexes_values\n",
    "    \n",
    "    df_results = pd.melt(means.reset_index(), id_vars=('index'), var_name='method', value_name='mean')\n",
    "    df_results['std'] = pd.melt(stds.reset_index(), id_vars=('index'), var_name='method', value_name='std')['std']\n",
    "    \n",
    "    return df_results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pROC_auc_rand(N, n_a, normalized=True):\n",
    "    x_1 = np.log10(1/N)\n",
    "    x_2 = 0\n",
    "    a = (10**x_1) / np.log(10)\n",
    "    b = (10**x_2) / np.log(10)\n",
    "    auc = (b - a)\n",
    "    \n",
    "    if normalized:\n",
    "        auc /= - x_1\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, shapiro, bartlett, mannwhitneyu \n",
    "\n",
    "def compare_two_distributions(conf):\n",
    "    actives = X_tian.query('activity == 1')[conf]\n",
    "    inactives = X_tian.query('activity == 0')[conf]\n",
    "    dk_scores = X_tian[[conf, 'activity']]\n",
    "\n",
    "    # Normality test\n",
    "    print('Actives Normality')\n",
    "    display(shapiro(actives))\n",
    "    print('Inactives Normality')\n",
    "    display(shapiro(inactives))\n",
    "    # T-test\n",
    "    print('t-student test')\n",
    "    display(ttest_ind(actives, inactives))\n",
    "    # Bartlett\n",
    "    print('Bartlett test')\n",
    "    display(bartlett(actives, inactives))\n",
    "    # AUC_ROC\n",
    "    print('AUC-ROC')\n",
    "    display(auc_all.loc[conf])\n",
    "    # Compute Wilcoxon\n",
    "    display(mannwhitneyu(actives, inactives))\n",
    "    \n",
    "    med_actives = np.median(actives)\n",
    "    med_inactives = np.median(inactives)\n",
    "\n",
    "\n",
    "    fig, (box, hist) = plt.subplots(2, sharex=True, figsize=(10, 8),\n",
    "                                   gridspec_kw= {\"height_ratios\": (0.2, 1)})\n",
    "    \n",
    "    box.set(title='Active/Inactive docking scores distribution')\n",
    "#     sns.boxplot(x=inactives, ax=box)\n",
    "    sns.boxplot(x=conf, y='activity', data=dk_scores, ax=box, orient='h')\n",
    "    box.set(xlabel=None, ylabel=None)\n",
    "    \n",
    "    hist = sns.distplot(inactives, \n",
    "                      label='Decoys/Inactives',hist_kws={'linewidth': 0}, norm_hist=True)\n",
    "    sns.distplot(actives, ax=hist, bins=35,\n",
    "                 label='Actives', hist_kws={'linewidth': 0}, norm_hist=True)\n",
    "    hist.axvline(med_actives, linestyle='--', c='#BD5D19')\n",
    "    hist.axvline(med_inactives, linestyle='--')\n",
    "    hist.set(xlabel=f'Conformation: {conf}', ylabel='Density')\n",
    "    \n",
    "    hist.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "import scikit_posthocs as sp \n",
    "\n",
    "def get_model_y_rand_results(vs_method, metric):\n",
    "    df_y_random.query('method == \"LR\"')\n",
    "    chis = chi_results.keys()\n",
    "    chi_vals = []\n",
    "    for chi in chi_results.keys():\n",
    "        chi_val = chi_results[chi].loc[metric][[vs_method]]\n",
    "        chi_vals.append(chi_val)\n",
    "    res = pd.concat(chi_vals, axis=1)\n",
    "    res.columns = chis\n",
    "    res = res.melt(var_name='Method', value_name='score').reset_index()\n",
    "    return res\n",
    "\n",
    "def get_pairwise_MW(vs_method, metric, vs_method_name):\n",
    "    new_names = dict(zip(chi_results.keys(), ['100%', '75%', '50%', '25%', '0%']))\n",
    "    df_y_res = get_model_y_rand_results(vs_method, metric)\n",
    "    df_res = pg.pairwise_ttests(dv='score', between='Method',  \n",
    "                   data=df_y_res, parametric=False).round(3) \n",
    "    df_res = df_res.drop(['Paired', 'Parametric'], axis=1)\n",
    "    df_res['A'] = df_res['A'].map(new_names)\n",
    "    df_res['B'] = df_res['B'].map(new_names)\n",
    "    df_res['Contrast'] = df_res['Contrast'].map({'Method': vs_method_name})\n",
    "    df_res['Signif.'] = pd.cut(df_res['p-unc'], \n",
    "                                    bins=(-np.inf, 0.001, 0.01, 0.05, np.inf),\n",
    "                                labels=['***','**','*','NS'])\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def _run_RFECV(estimator, X_train, y_train, cv=5, \n",
    "               scoring='roc_auc', **kwargs):\n",
    "    '''Simply runs a RFECV procedure'''\n",
    "    # Intance and fit the rfe selector\n",
    "    selector = RFECV(estimator,  cv = cv,\n",
    "                     scoring = scoring, **kwargs)\n",
    "    \n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    return selector\n",
    "\n",
    "def get_selected_features_per_step(fitted_selector, X):\n",
    "    # Get the features ranking\n",
    "    df_ = pd.DataFrame({'pdb_id': X.columns, 'rfe_ranking': fitted_selector.ranking_})\n",
    "    # Sort features by ranking\n",
    "    df_.sort_values('rfe_ranking', inplace = True)\n",
    "    # Create a list of ranked features from size 1 to n\n",
    "    list_of_confs_per_k = [ df_.index[:i+1].tolist() for i in range(len(df_))]\n",
    "    # Create a dataframe indicating which features belong to a k subset\n",
    "    df_confs_per_k = pd.DataFrame({'list_of_confs_rfe': list_of_confs_per_k})\n",
    "    return df_confs_per_k\n",
    "\n",
    "@run_or_load_joblib\n",
    "def REFCV_wrapper(filename, estimator, X_train, y_train,\n",
    "                  cv=5, scoring='roc_auc', **kwargs):\n",
    "    estimator = _run_RFECV(estimator, X_train, y_train, cv=cv, \n",
    "                           scoring=scoring, **kwargs)\n",
    "    return estimator\n",
    "\n",
    "\n",
    "@run_or_load_joblib\n",
    "def RFE_wrapper(filename, X, y, **kwargs):\n",
    "    estimator = RFE(**kwargs)\n",
    "    estimator.fit(X, y)\n",
    "    return estimator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
